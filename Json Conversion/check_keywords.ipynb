{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e796058",
   "metadata": {},
   "source": [
    "Evaluate earning transcipts for signals of financial distress\n",
    "\n",
    "Code by Geoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbfd3df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from python-docx) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in e:\\anaconda\\lib\\site-packages (from python-docx) (4.11.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: torch in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\anaconda\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: setuptools in e:\\anaconda\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (3.1.4)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\hijik\\appdata\\roaming\\python\\python312\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "install_flag = True\n",
    "if install_flag:\n",
    "  !pip install python-docx\n",
    "  !pip install transformers\n",
    "  !pip install torch\n",
    "  !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b58e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, os, re\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Any, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import openai\n",
    "import requests\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class AnalysisResult:\n",
    "    \"\"\"\n",
    "    📊 Data structure to hold financial distress analysis results\n",
    "    \n",
    "    Attributes:\n",
    "        prompt_name: Name of the financial distress indicator being tested\n",
    "        category: Category of the distress indicator (e.g., \"Management and Governance Issues\")\n",
    "        keywords_found: List of keywords detected in the text\n",
    "        context_matches: List of contextual phrases that match the indicator\n",
    "        confidence_score: Float 0-100 indicating confidence in the finding\n",
    "        reasoning: Detailed explanation of why this indicator was flagged\n",
    "        source_file: Path to the file where this indicator was found\n",
    "    \"\"\"\n",
    "    prompt_name: str\n",
    "    category: str\n",
    "    keywords_found: List[str]\n",
    "    context_matches: List[str]\n",
    "    confidence_score: float\n",
    "    reasoning: str\n",
    "    source_file: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dedaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLMProvider(ABC):\n",
    "    \"\"\"\n",
    "    🤖 Abstract base class for Large Language Model providers\n",
    "    \n",
    "    This enables switching between different LLMs (ChatGPT, Phi4, etc.)\n",
    "    while maintaining the same interface for financial analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def analyze_text(self, prompt: str, text: str) -> str:\n",
    "        \"\"\"\n",
    "        🔍 Analyze financial text using the specific LLM implementation\n",
    "        \n",
    "        Args:\n",
    "            prompt: The structured analysis prompt containing distress indicators\n",
    "            text: The financial document text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            LLM's analysis response as a string\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_provider_name(self) -> str:\n",
    "        \"\"\"\n",
    "        📛 Return the name of the LLM provider for reporting purposes\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChatGPTProvider(LLMProvider):\n",
    "    \"\"\"\n",
    "    🤖 ChatGPT implementation for financial distress analysis\n",
    "    \n",
    "    Uses OpenAI's API to analyze financial documents for distress indicators.\n",
    "    Optimized for accurate, structured financial analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4\"):\n",
    "        \"\"\"\n",
    "        🔧 Initialize ChatGPT provider with API credentials\n",
    "        \n",
    "        Args:\n",
    "            api_key: OpenAI API key for authentication\n",
    "            model: Model to use (default: gpt-4 for best analysis quality)\n",
    "        \"\"\"\n",
    "        self.client = openai.OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "    \n",
    "    def analyze_text(self, prompt: str, text: str) -> str:\n",
    "        \"\"\"\n",
    "        💬 Analyze financial text using ChatGPT's advanced reasoning capabilities\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a financial analyst expert in detecting signs of financial distress in banking institutions. Provide structured, evidence-based analysis.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nText to analyze:\\n{text}\"}\n",
    "                ],\n",
    "                max_tokens=1000,\n",
    "                temperature=0.1  # Low temperature for consistent, factual analysis\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"❌ Error analyzing with ChatGPT: {str(e)}\"\n",
    "    \n",
    "    def get_provider_name(self) -> str:\n",
    "        return \"🤖 ChatGPT\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dffdf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Phi4Provider(LLMProvider):\n",
    "    \"\"\"\n",
    "    🧠 Microsoft Phi4 implementation for financial analysis using Hugging Face Transformers\n",
    "    \n",
    "    Uses local inference via Hugging Face Transformers library for privacy and control.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"microsoft/Phi-4\"):\n",
    "        \"\"\"\n",
    "        ⚙️ Initialize Phi4 provider with Hugging Face Transformers\n",
    "        \n",
    "        Args:\n",
    "            model_name: Hugging Face model identifier (default: microsoft/Phi-4)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        # 📦 Load Hugging Face transformers for local inference\n",
    "        try:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "            print(f\"🔄 Loading Phi4 model: {model_name}\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "            print(f\"✅ Successfully loaded Phi4 model: {model_name}\")\n",
    "        except ImportError:\n",
    "            raise ImportError(\"❌ transformers library not found. Install with: pip install transformers torch\")\n",
    "    \n",
    "    def analyze_text(self, prompt: str, text: str) -> str:\n",
    "        \"\"\"\n",
    "        🔍 Analyze financial text using Phi4's reasoning capabilities\n",
    "        \"\"\"\n",
    "        full_prompt = f\"{prompt}\\n\\nText to analyze:\\n{text}\"\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer.encode(full_prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs,\n",
    "                    max_new_tokens=500,\n",
    "                    temperature=0.1,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # Remove the original prompt from the response\n",
    "            return response[len(full_prompt):].strip()\n",
    "        except Exception as e:\n",
    "            return f\"❌ Error analyzing with Phi4: {str(e)}\"\n",
    "    \n",
    "    def get_provider_name(self) -> str:\n",
    "        return f\"🧠 Phi4 ({self.model_name})\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FinancialDistressAnalyzer:\n",
    "    \"\"\"\n",
    "    🏦 Main analyzer class for detecting financial distress signals in banking documents\n",
    "    \n",
    "    This class orchestrates the entire analysis process:\n",
    "    - 📝 Loads financial distress indicator prompts\n",
    "    - 📄 Processes text and Excel files \n",
    "    - 🔍 Applies intelligent text chunking for large documents\n",
    "    - 🤖 Uses LLMs to analyze content for distress indicators\n",
    "    - 📊 Generates comprehensive risk assessment reports\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_provider: LLMProvider, max_tokens: int = 8000):\n",
    "        \"\"\"\n",
    "        🚀 Initialize the financial distress analyzer\n",
    "        \n",
    "        Args:\n",
    "            llm_provider: The LLM provider instance (ChatGPT, Phi4, etc.)\n",
    "            max_tokens: Maximum tokens per analysis chunk (default: 8000 for safety)\n",
    "        \"\"\"\n",
    "        self.llm_provider = llm_provider\n",
    "        self.prompts = []\n",
    "        self.results = []\n",
    "        self.max_tokens = max_tokens  # Safe token limit to prevent API errors\n",
    "    \n",
    "    def load_prompts(self, prompts_file_path: str):\n",
    "        \"\"\"\n",
    "        📋 Load financial distress interrogation prompts from JSON file\n",
    "        \n",
    "        The prompts file contains structured indicators across categories like:\n",
    "        - 🏛️ Management and Governance Issues\n",
    "        - 📈 Market and External Perceptions  \n",
    "        - 🏢 Business Model and Strategic Shifts\n",
    "        \n",
    "        Args:\n",
    "            prompts_file_path: Path to the JSON file containing distress indicators\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(prompts_file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                self.prompts = data.get('prompts', [])\n",
    "            print(f\"✅ Loaded {len(self.prompts)} financial distress indicators successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading prompts: {str(e)}\")\n",
    "    \n",
    "    def chunk_text(self, text: str, max_chars: int = 6000) -> List[str]:\n",
    "        \"\"\"\n",
    "        ✂️ Intelligently split large documents into manageable chunks\n",
    "        \n",
    "        Uses a sophisticated chunking strategy that:\n",
    "        1. 📖 Preserves paragraph boundaries when possible\n",
    "        2. 📝 Falls back to sentence boundaries if needed\n",
    "        3. 🔗 Maintains context and readability across chunks\n",
    "        \n",
    "        Args:\n",
    "            text: The input text to be chunked\n",
    "            max_chars: Maximum characters per chunk (≈1,500 tokens)\n",
    "            \n",
    "        Returns:\n",
    "            List of text chunks ready for LLM analysis\n",
    "        \"\"\"\n",
    "        if len(text) <= max_chars:\n",
    "            return [text]  # No chunking needed\n",
    "        \n",
    "        chunks = []\n",
    "        # Try to split at natural boundaries (paragraphs first, then sentences)\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            # If adding this paragraph would exceed limit, save current chunk\n",
    "            if len(current_chunk) + len(paragraph) > max_chars and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = paragraph\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    current_chunk += \"\\n\\n\" + paragraph\n",
    "                else:\n",
    "                    current_chunk = paragraph\n",
    "            \n",
    "            # If even a single paragraph is too long, split it at sentence boundaries\n",
    "            if len(current_chunk) > max_chars:\n",
    "                sentences = current_chunk.split('. ')\n",
    "                temp_chunk = \"\"\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                    if len(temp_chunk) + len(sentence) > max_chars and temp_chunk:\n",
    "                        chunks.append(temp_chunk.strip())\n",
    "                        temp_chunk = sentence\n",
    "                    else:\n",
    "                        if temp_chunk:\n",
    "                            temp_chunk += \". \" + sentence\n",
    "                        else:\n",
    "                            temp_chunk = sentence\n",
    "                \n",
    "                current_chunk = temp_chunk\n",
    "        \n",
    "        # Add the final chunk if it contains content\n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def analyze_text_chunks(self, text: str, prompt_data: Dict[str, Any], source_file: str) -> AnalysisResult:\n",
    "        \"\"\"\n",
    "        🔍 Analyze text by breaking it into chunks and combining results\n",
    "        \n",
    "        For large documents that exceed token limits, this method:\n",
    "        1. 📊 Splits text into manageable chunks\n",
    "        2. 🤖 Analyzes each chunk with the LLM\n",
    "        3. 🔗 Intelligently combines findings across all chunks\n",
    "        4. 📈 Takes the highest confidence score found\n",
    "        \n",
    "        Args:\n",
    "            text: Full text content to analyze\n",
    "            prompt_data: Financial distress indicator configuration\n",
    "            source_file: Name of the source file being analyzed\n",
    "            \n",
    "        Returns:\n",
    "            Combined analysis result from all chunks\n",
    "        \"\"\"\n",
    "        chunks = self.chunk_text(text)\n",
    "        \n",
    "        if len(chunks) == 1:\n",
    "            # Single chunk - analyze normally without chunking overhead\n",
    "            analysis_prompt = self.create_analysis_prompt(prompt_data)\n",
    "            llm_response = self.llm_provider.analyze_text(analysis_prompt, text)\n",
    "            return self._parse_llm_response(llm_response, prompt_data, source_file)\n",
    "        \n",
    "        # Multiple chunks - analyze each and intelligently combine results\n",
    "        print(f\"  📄 Document split into {len(chunks)} chunks for analysis\")\n",
    "        \n",
    "        all_keywords_found = []\n",
    "        all_context_matches = []\n",
    "        max_confidence = 0.0\n",
    "        combined_reasoning = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"    🔍 Analyzing chunk {i+1}/{len(chunks)}\")\n",
    "            \n",
    "            analysis_prompt = self.create_analysis_prompt(prompt_data)\n",
    "            # Add context about chunking to help LLM understand the analysis scope\n",
    "            chunk_prompt = f\"{analysis_prompt}\\n\\nNote: This is chunk {i+1} of {len(chunks)} from a larger document.\"\n",
    "            \n",
    "            llm_response = self.llm_provider.analyze_text(chunk_prompt, chunk)\n",
    "            chunk_result = self._parse_llm_response(llm_response, prompt_data, f\"{source_file} (chunk {i+1})\")\n",
    "            \n",
    "            # Combine results across chunks\n",
    "            all_keywords_found.extend(chunk_result.keywords_found)\n",
    "            all_context_matches.extend(chunk_result.context_matches)\n",
    "            max_confidence = max(max_confidence, chunk_result.confidence_score)\n",
    "            \n",
    "            # Only include reasoning from chunks with significant findings\n",
    "            if chunk_result.reasoning and chunk_result.confidence_score > 20:\n",
    "                combined_reasoning.append(f\"Chunk {i+1}: {chunk_result.reasoning}\")\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        all_keywords_found = list(set(all_keywords_found))\n",
    "        all_context_matches = list(set(all_context_matches))\n",
    "        \n",
    "        # Combine reasoning from all significant findings\n",
    "        final_reasoning = \"; \".join(combined_reasoning) if combined_reasoning else \"No significant indicators found across document chunks\"\n",
    "        \n",
    "        return AnalysisResult(\n",
    "            prompt_name=prompt_data['name'],\n",
    "            category=prompt_data['category'],\n",
    "            keywords_found=all_keywords_found,\n",
    "            context_matches=all_context_matches,\n",
    "            confidence_score=max_confidence,\n",
    "            reasoning=final_reasoning,\n",
    "            source_file=source_file\n",
    "        )\n",
    "    \n",
    "    def create_analysis_prompt(self, prompt_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        📝 Create a structured, comprehensive prompt for LLM financial analysis\n",
    "        \n",
    "        This method builds a detailed prompt that:\n",
    "        - 🎯 Focuses the LLM on specific financial distress indicators\n",
    "        - 📊 Provides context about what to look for\n",
    "        - 📋 Requests structured output for consistent parsing\n",
    "        - 🔍 Guides the LLM to provide evidence-based analysis\n",
    "        \n",
    "        Args:\n",
    "            prompt_data: Dictionary containing the financial distress indicator configuration\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt string ready for LLM analysis\n",
    "        \"\"\"\n",
    "        prompt_template = f\"\"\"\n",
    "FINANCIAL DISTRESS ANALYSIS TASK\n",
    "\n",
    "You are analyzing financial documents for signs of banking distress. Focus on this specific indicator:\n",
    "\n",
    "**Category**: {prompt_data['category']}\n",
    "**Indicator**: {prompt_data['name']}\n",
    "\n",
    "**Keywords to search for**: {', '.join(prompt_data['keywords'])}\n",
    "\n",
    "**Context**: {prompt_data.get('context', 'General financial distress indicator - look for any signs of this issue in the document')}\n",
    "\n",
    "**ANALYSIS INSTRUCTIONS**:\n",
    "1. Carefully search the provided text for any of the specified keywords or related concepts\n",
    "2. Identify specific phrases, sentences, or passages that suggest this type of financial distress\n",
    "3. Provide a confidence score (0-100) indicating how strongly the text suggests this distress indicator\n",
    "4. Explain your reasoning, citing specific text passages as evidence\n",
    "5. If no clear evidence is found, state this explicitly with reasoning\n",
    "\n",
    "**REQUIRED RESPONSE FORMAT**:\n",
    "- KEYWORDS_FOUND: [list any keywords or related terms found]\n",
    "- CONFIDENCE_SCORE: [0-100 where 0=no evidence, 100=strong evidence]\n",
    "- EVIDENCE: [specific text passages that support your assessment]\n",
    "- REASONING: [detailed explanation of your analysis and why you assigned this confidence score]\n",
    "- CONCLUSION: [clear summary of whether this distress indicator is present and at what level]\n",
    "\n",
    "Focus on accuracy and evidence-based analysis. Be specific about what you found and why it matters.\n",
    "\"\"\"\n",
    "        return prompt_template\n",
    "    \n",
    "    def analyze_text_file(self, file_path: str) -> List[AnalysisResult]:\n",
    "        \"\"\"\n",
    "        📄 Analyze a text file for financial distress indicators\n",
    "        \n",
    "        Processes financial documents (like earnings presentations, reports) to detect\n",
    "        signs of banking distress across all loaded indicator categories.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the text file to analyze\n",
    "            \n",
    "        Returns:\n",
    "            List of analysis results for each financial distress indicator\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text_content = f.read()\n",
    "            \n",
    "            print(f\"📄 Text file size: {len(text_content)} characters\")\n",
    "            \n",
    "            results = []\n",
    "            \n",
    "            for i, prompt_data in enumerate(self.prompts):\n",
    "                print(f\"🔍 Processing prompt {i+1}/{len(self.prompts)}: {prompt_data['name']}\")\n",
    "                \n",
    "                # Use intelligent chunked analysis to handle large documents\n",
    "                result = self.analyze_text_chunks(text_content, prompt_data, file_path)\n",
    "                results.append(result)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error analyzing text file: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def analyze_excel_file(self, file_path: str) -> List[AnalysisResult]:\n",
    "        \"\"\"\n",
    "        📊 Analyze an Excel file for financial distress indicators\n",
    "        \n",
    "        Converts Excel data to text format and processes it through the same\n",
    "        analysis pipeline as text files, handling multiple sheets.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the Excel file to analyze\n",
    "            \n",
    "        Returns:\n",
    "            List of analysis results for each financial distress indicator\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read all sheets from Excel file\n",
    "            excel_data = pd.read_excel(file_path, sheet_name=None)\n",
    "            \n",
    "            # Combine all sheets into text for analysis\n",
    "            combined_text = \"\"\n",
    "            for sheet_name, df in excel_data.items():\n",
    "                combined_text += f\"\\n--- Sheet: {sheet_name} ---\\n\"\n",
    "                combined_text += df.to_string(index=False)\n",
    "                combined_text += \"\\n\"\n",
    "            \n",
    "            print(f\"📊 Excel file size: {len(combined_text)} characters\")\n",
    "            \n",
    "            results = []\n",
    "            \n",
    "            for i, prompt_data in enumerate(self.prompts):\n",
    "                print(f\"🔍 Processing Excel prompt {i+1}/{len(self.prompts)}: {prompt_data['name']}\")\n",
    "                \n",
    "                # Use chunked analysis for Excel data as well\n",
    "                result = self.analyze_text_chunks(combined_text, prompt_data, file_path)\n",
    "                results.append(result)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error analyzing Excel file: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _parse_llm_response(self, response: str, prompt_data: Dict[str, Any], source_file: str) -> AnalysisResult:\n",
    "        \"\"\"\n",
    "        🔧 Parse LLM response into structured AnalysisResult object\n",
    "        \n",
    "        Extracts structured information from the LLM's text response using regex patterns\n",
    "        to identify keywords found, confidence scores, and reasoning.\n",
    "        \n",
    "        Args:\n",
    "            response: Raw text response from the LLM\n",
    "            prompt_data: Original prompt configuration data\n",
    "            source_file: Source file being analyzed\n",
    "            \n",
    "        Returns:\n",
    "            Structured AnalysisResult object with parsed information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract information using regex patterns\n",
    "            keywords_found = self._extract_list_from_response(response, \"KEYWORDS_FOUND\")\n",
    "            confidence_score = self._extract_confidence_score(response)\n",
    "            reasoning = self._extract_section_from_response(response, \"REASONING\")\n",
    "            \n",
    "            # Find context matches in the response\n",
    "            context_matches = []\n",
    "            for keyword in prompt_data['keywords']:\n",
    "                if keyword.lower() in response.lower():\n",
    "                    context_matches.append(keyword)\n",
    "            \n",
    "            return AnalysisResult(\n",
    "                prompt_name=prompt_data['name'],\n",
    "                category=prompt_data['category'],\n",
    "                keywords_found=keywords_found,\n",
    "                context_matches=context_matches,\n",
    "                confidence_score=confidence_score,\n",
    "                reasoning=reasoning,\n",
    "                source_file=source_file\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return AnalysisResult(\n",
    "                prompt_name=prompt_data['name'],\n",
    "                category=prompt_data['category'],\n",
    "                keywords_found=[],\n",
    "                context_matches=[],\n",
    "                confidence_score=0.0,\n",
    "                reasoning=f\"Error parsing response: {str(e)}\",\n",
    "                source_file=source_file\n",
    "            )\n",
    "    \n",
    "    def _extract_list_from_response(self, response: str, section_name: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        📝 Extract list items from structured LLM response sections\n",
    "        \n",
    "        Uses regex to find and parse list items from sections like KEYWORDS_FOUND.\n",
    "        \n",
    "        Args:\n",
    "            response: LLM response text\n",
    "            section_name: Name of the section to extract (e.g., \"KEYWORDS_FOUND\")\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted items, cleaned and filtered\n",
    "        \"\"\"\n",
    "        pattern = f\"{section_name}:\\\\s*(.*?)(?=\\\\n[A-Z_]+:|$)\"\n",
    "        match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            items_text = match.group(1).strip()\n",
    "            # Parse list items, handling various formats\n",
    "            items = [item.strip().strip('[]') for item in items_text.split(',')]\n",
    "            return [item for item in items if item and item != 'None']\n",
    "        return []\n",
    "    \n",
    "    def _extract_confidence_score(self, response: str) -> float:\n",
    "        \"\"\"\n",
    "        📊 Extract confidence score from LLM response\n",
    "        \n",
    "        Searches for CONFIDENCE_SCORE pattern and extracts the numeric value.\n",
    "        \n",
    "        Args:\n",
    "            response: LLM response text\n",
    "            \n",
    "        Returns:\n",
    "            Confidence score as float (0.0-100.0)\n",
    "        \"\"\"\n",
    "        pattern = r\"CONFIDENCE_SCORE:\\s*(\\d+(?:\\.\\d+)?)\"\n",
    "        match = re.search(pattern, response, re.IGNORECASE)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "        return 0.0\n",
    "    \n",
    "    def _extract_section_from_response(self, response: str, section_name: str) -> str:\n",
    "        \"\"\"\n",
    "        📖 Extract a specific section's content from LLM response\n",
    "        \n",
    "        Uses regex to find and extract content from named sections like REASONING.\n",
    "        \n",
    "        Args:\n",
    "            response: LLM response text\n",
    "            section_name: Name of the section to extract\n",
    "            \n",
    "        Returns:\n",
    "            Section content as cleaned string\n",
    "        \"\"\"\n",
    "        pattern = f\"{section_name}:\\\\s*(.*?)(?=\\\\n[A-Z_]+:|$)\"\n",
    "        match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def generate_report(self, results: List[AnalysisResult], output_file: str = \"financial_distress_report.txt\"):\n",
    "        \"\"\"\n",
    "        📋 Generate a comprehensive financial distress analysis report\n",
    "        \n",
    "        Creates a detailed report categorizing findings by risk level and providing\n",
    "        executive summary, detailed findings, and actionable insights.\n",
    "        \n",
    "        Args:\n",
    "            results: List of analysis results from all indicators\n",
    "            output_file: Path where the report should be saved\n",
    "            \n",
    "        Returns:\n",
    "            Generated report text (also saved to file)\n",
    "        \"\"\"\n",
    "        high_risk_indicators = [r for r in results if r.confidence_score >= 70]\n",
    "        medium_risk_indicators = [r for r in results if 30 <= r.confidence_score < 70]\n",
    "        \n",
    "        report = f\"\"\"\n",
    "=== FINANCIAL DISTRESS ANALYSIS REPORT ===\n",
    "LLM Provider: {self.llm_provider.get_provider_name()}\n",
    "Total Indicators Analyzed: {len(results)}\n",
    "\n",
    "=== EXECUTIVE SUMMARY ===\n",
    "🔴 High Risk Indicators (70-100): {len(high_risk_indicators)}\n",
    "🟡 Medium Risk Indicators (30-69): {len(medium_risk_indicators)}\n",
    "🟢 Low Risk Indicators (0-29): {len(results) - len(high_risk_indicators) - len(medium_risk_indicators)}\n",
    "\n",
    "=== HIGH RISK INDICATORS ===\n",
    "\"\"\"\n",
    "        \n",
    "        for result in high_risk_indicators:\n",
    "            report += f\"\"\"\n",
    "🔴 Indicator: {result.prompt_name}\n",
    "📊 Category: {result.category}\n",
    "📈 Confidence Score: {result.confidence_score}\n",
    "📁 Source: {result.source_file}\n",
    "🔍 Keywords Found: {', '.join(result.keywords_found) if result.keywords_found else 'None'}\n",
    "💭 Reasoning: {result.reasoning}\n",
    "---\n",
    "\"\"\"\n",
    "        \n",
    "        report += \"\\n=== MEDIUM RISK INDICATORS ===\\n\"\n",
    "        \n",
    "        for result in medium_risk_indicators:\n",
    "            report += f\"\"\"\n",
    "🟡 Indicator: {result.prompt_name}\n",
    "📊 Category: {result.category}\n",
    "📈 Confidence Score: {result.confidence_score}\n",
    "📁 Source: {result.source_file}\n",
    "🔍 Keywords Found: {', '.join(result.keywords_found) if result.keywords_found else 'None'}\n",
    "---\n",
    "\"\"\"\n",
    "        \n",
    "        # Save report to file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"📋 Report saved to {output_file}\")\n",
    "        return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=== Financial Distress Analysis Tool ===\")\n",
    "    \n",
    "    # Choose LLM provider\n",
    "    print(\"\\nAvailable LLM Providers:\")\n",
    "    print(\"1. ChatGPT\")\n",
    "    print(\"2. Phi4\")\n",
    "    \n",
    "    choice = input(\"Select LLM provider (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        api_key = os.environ.get(\"OPENAI_API_KEY\")  # Use environment variable if available\n",
    "        model = \"gpt-4\"\n",
    "        llm_provider = ChatGPTProvider(api_key, model)\n",
    "    elif choice == \"2\":\n",
    "        model_name = \"microsoft/Phi-4\"\n",
    "        method = \"transformers\"       \n",
    "        llm_provider = Phi4Provider(model_name, method)\n",
    "    else:\n",
    "        print(\"Invalid choice. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = FinancialDistressAnalyzer(llm_provider)\n",
    "    \n",
    "    # Load prompts\n",
    "    prompts_file = input(\"Enter path to prompts JSON file: \").strip()\n",
    "    analyzer.load_prompts(prompts_file)\n",
    "    \n",
    "    if not analyzer.prompts:\n",
    "        print(\"No prompts loaded. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Analyze files\n",
    "    all_results = []\n",
    "    \n",
    "    # Analyze text file\n",
    "    text_file = input(\"Enter path to text file (JPM presentation): \").strip()\n",
    "    if text_file:\n",
    "        print(f\"\\nAnalyzing text file: {text_file}\")\n",
    "        text_results = analyzer.analyze_text_file(text_file)\n",
    "        all_results.extend(text_results)\n",
    "    \n",
    "    # Analyze Excel file\n",
    "    excel_file = input(\"Enter path to Excel file (JPM Q&A data): \").strip()\n",
    "    if excel_file:\n",
    "        print(f\"\\nAnalyzing Excel file: {excel_file}\")\n",
    "        excel_results = analyzer.analyze_excel_file(excel_file)\n",
    "        all_results.extend(excel_results)\n",
    "    \n",
    "    # Generate report\n",
    "    if all_results:\n",
    "        print(f\"\\nGenerating analysis report...\")\n",
    "        report = analyzer.generate_report(all_results)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total indicators analyzed: {len(all_results)}\")\n",
    "        print(f\"High risk indicators found: {len([r for r in all_results if r.confidence_score >= 70])}\")\n",
    "        print(\"Report saved to: financial_distress_report.txt\")\n",
    "    else:\n",
    "        print(\"No results to analyze.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
